# Constitutional AI from Base Models: Experiment Report

**Authors:** Thinking Machines Lab
**Date:** December 29, 2025
**Status:** Complete with Caveats

---

## Abstract

We replicate Constitutional AI (CAI) training starting from a base language model rather than an instruction-tuned model, following the methodology outlined by Bai et al. (2022). Our implementation trains a helpful-only model via supervised fine-tuning on a Llama 3.2-3B base model, generates constitutional critique/revision pairs using few-shot prompting, and performs Direct Preference Optimization (DPO) to instill constitutional principles. The resulting model achieves a **50.3% reduction in Attack Success Rate** compared to the SFT-only baseline while maintaining high helpfulness (4.45/5.0). However, our style diversity evaluation comparing CAI to instruction-tuned models contains **critical methodological flaws** that invalidate the comparison results.

---

## 1. Introduction

### 1.1 Motivation

Reinforcement Learning from AI Feedback (RLAIF) has become a standard approach for aligning language models. However, most implementations bootstrap from existing instruction-tuned models, introducing an implicit dependence on the model used for data generation. As noted in the original project specification:

> "RL from AI feedback is widely used, people mostly bootstrap off of existing instruction-tuned models. This introduces a strong implicit dependence on the model used for data generation."

This work aims to implement Constitutional AI training "starting with a base model and doesn't use an instruction-tuned model as part of the pipeline," enabling cleaner study of constitutional principles without "contamination" by existing assistant models.

### 1.2 Hypotheses

**Primary Hypothesis (H1):** CAI training from a base model can produce models that are both helpful and harmless, as measured by Attack Success Rate (ASR) reduction and helpfulness scores.

**Secondary Hypothesis (H2):** CAI models trained from base models would be "more base model-like in interesting ways," specifically demonstrating improved capability at "writing in different styles that are hard to steer existing instruction-tuned models to follow."

---

## 2. Methods

### 2.1 Training Pipeline

We implement the full CAI pipeline from Bai et al. (2022):

1. **Supervised Fine-Tuning (SFT):** Train a helpful-only assistant from the base model using curated helpful instruction-response pairs.

2. **Constitutional Data Generation:** Generate critique and revision pairs using few-shot prompting with constitutional principles. Each response undergoes multiple revision rounds.

3. **Direct Preference Optimization (DPO):** Train the model to prefer revised (constitutional) responses over original responses.

### 2.2 Model Configuration

| Parameter | Value |
|-----------|-------|
| Base Model | `meta-llama/Llama-3.2-3B` |
| LoRA Rank | 32 |
| SFT Steps | 200 |
| DPO Steps | 300 |
| Learning Rate | 1e-5 |
| Revision Rounds | 2 |
| Seeds | 8 (0, 2, 3, 4, 5, 7, 8, 9) |

### 2.3 Constitutional Principles

We use a constitution derived from Anthropic's published principles, covering:
- Harmlessness and safety
- Honesty and accuracy
- Helpfulness without harm
- Appropriate refusal of dangerous requests

### 2.4 Evaluation

**Harmlessness Evaluation:**
- Attack Success Rate (ASR) on red-team prompts
- Evaluated using a 70B instruction-tuned judge model (`meta-llama/Llama-3.3-70B-Instruct`)

**Helpfulness Evaluation:**
- Multi-criteria scoring (Clarity, Completeness, Accuracy, Relevance, Tone)
- Overall helpfulness score (1-5 scale)

**Style Diversity Evaluation:**
- 13 writing styles tested (formal academic, casual, poetic, technical, etc.)
- Style adherence scored by judge model
- Comparison against instruction-tuned baseline

---

## 3. Results

### 3.1 Primary Results: CAI Effectiveness (H1) ‚úì

The CAI training successfully reduces harmful outputs while maintaining helpfulness:

| Metric | SFT-Only | Full CAI | Œî |
|--------|----------|----------|---|
| Attack Success Rate | 84.9% ¬± 4.1% | 42.2% ¬± 16.9% | **-50.3%** |
| Helpfulness (1-5) | 4.90 ¬± 0.10 | 4.45 ¬± 0.24 | -0.45 |
| Harmlessness Score | 15.1% | 57.8% | **+42.7%** |

**Key Finding:** CAI training achieves a **50.3% relative reduction** in Attack Success Rate with only a modest decrease in helpfulness (-0.45 on a 5-point scale). This demonstrates that constitutional training from base models is effective.

#### Training Dynamics

| Phase | Final Loss (Mean ¬± Std) |
|-------|-------------------------|
| SFT | 0.0081 ¬± 0.0002 |
| DPO | 0.141 ¬± 0.026 |

### 3.2 Style Diversity Results (H2) ‚ö†Ô∏è INVALID

**CRITICAL METHODOLOGICAL ISSUE:** The style diversity comparison produced invalid results due to a failure in the instruction-tuned baseline model.

#### Raw Results (Presented for Transparency)

| Metric | CAI | "Instruct" | Œî |
|--------|-----|------------|---|
| Style Wins | 12 | 0 | - |
| Ties | 1 | - | - |
| Mean Adherence | 2.31 | 1.00 | +1.31 |
| Distinctiveness | 2177.1 | 1055.2 | +1121.9 |

#### Why These Results Are Invalid

Examination of the instruction-tuned model (`Qwen/Qwen3-4B-Instruct-2507`) outputs reveals **complete generation failures**:

```
# Example "instruct" model outputs (verbatim):

Style: technical_precise
Prompt: "Explain how to back up files."
Response: "toatform, 1) 414_ for against, 2) for414_ for against..."

Style: storytelling_narrative
Prompt: "Describe the first moon landing."
Response: "SESmer of the is is is l discussion, re. 4) 5) 6) 7)..."

Style: minimalist_terse
Prompt: "Explain democracy."
Response: "(error of theAuthor of, andName the 50Valid..."
```

The instruction-tuned model produced **nonsensical, repetitive gibberish** rather than coherent responses. This appears to be caused by:

1. **Tokenizer/template mismatch:** The model may not have received properly formatted chat templates
2. **Prompt format incompatibility:** The few-shot prompt format designed for the CAI (base) model was applied to the instruction-tuned model
3. **Possible model loading error:** The Qwen model may not have loaded correctly

**Conclusion for H2:** The style diversity comparison is scientifically invalid. The instruction-tuned baseline scores of 1.0 across all styles reflect **parsing failures on garbage outputs**, not genuine style inflexibility. **We cannot draw any conclusions about style diversity from this data.**

---

## 4. Discussion

### 4.1 Successful Replication of CAI Core Methodology

The primary contribution of this work is a successful replication of Constitutional AI training from a base model:

- **No instruction-tuned model contamination:** The entire pipeline operates from `Llama-3.2-3B` base model
- **Effective harm reduction:** 50.3% ASR reduction demonstrates constitutional principles transfer effectively
- **Preserved helpfulness:** The model remains useful (4.45/5.0) after safety training

### 4.2 Limitations

1. **Invalid Style Comparison:** The secondary hypothesis (H2) about style flexibility cannot be evaluated due to baseline model failures.

2. **Small Base Model:** 3B parameters limits absolute capability. Larger base models may show different dynamics.

3. **Limited Red-Team Coverage:** Our red-team prompts may not cover all harmful categories.

4. **Single Model Family:** Results are demonstrated only on Llama 3.2; generalization to other architectures is unknown.

### 4.3 Fixes Applied (2025-12-29)

#### Fix 1: Chat Template Handling

The style diversity evaluation originally failed because the instruction-tuned model produced garbage outputs. The following changes were made:

**1. `style_diversity_eval.py`:**
- Added `is_instruct` parameter to `evaluate_style()` and `full_evaluation()`
- When `is_instruct=True`, uses `apply_chat_template()` instead of `format_few_shot_prompt()`
- Added `_is_garbage_response()` method to detect malformed outputs:
  - Checks for low unique word ratio (<15%)
  - Detects repetitive numbered sequences
  - Catches repetitive patterns like repeated words or conversation markers
  - Flags garbage outputs with score 0.0

**2. `run_style_eval.py`:**
- Now loads separate tokenizers for CAI and instruct models
- Properly configures Llama 3 Instruct chat template
- Adds fallback Qwen chat template if not present
- Changed default instruct model to `Qwen/Qwen3-4B-Instruct-2507` (Tinker-supported)
- Passes `is_instruct=False` for CAI model, `is_instruct=True` for instruct model

#### Fix 2: Checkpoint Export (2025-12-29)

A second issue was discovered: the style evaluation compared **raw base model** vs instruction-tuned model, NOT the CAI-trained model. This happened because:

1. The Tinker API stores trained weights remotely via `save_weights_and_get_sampling_client()`
2. These remote weights are ephemeral - they don't persist after the training session ends
3. The original training run didn't export checkpoints locally
4. The style evaluation script couldn't find any checkpoints and fell back to using the raw base model

**Fixes applied to `cai_trainer.py`:**
- Added `export_checkpoint()` method to download trained weights from Tinker using `download_checkpoint_archive_from_tinker_path()`
- Added `export_all_checkpoints()` to export both SFT baseline and DPO final checkpoints
- Modified `train()` to accept `export_checkpoints` and `checkpoint_dir` parameters
- Checkpoints are now saved as `.tar.gz` archives containing LoRA adapter files

**Fixes applied to `run_experiment.py`:**
- Added `--export-checkpoints` flag to enable checkpoint export during training
- Checkpoints are saved to `results/<run_id>/seed_<N>/checkpoint/dpo_final.tar.gz`

**Fixes applied to `run_style_eval.py`:**
- Added support for tar.gz checkpoint archives
- Added `extract_checkpoint_archive()` helper function
- Improved checkpoint discovery with multiple fallback locations

**To run a new experiment with checkpoint export:**
```bash
cd cai-base-model
python run_experiment.py --n-seeds 1 --sft-steps 500 --dpo-steps 500 --export-checkpoints --output-dir results/cai_with_checkpoints
```

**To run style evaluation after training:**
```bash
cd cai-base-model
python run_style_eval.py --results-dir results/cai_with_checkpoints --output-dir results/style_eval_valid
```

### 4.4 Future Work

Additional improvements could include:

1. **Human evaluation:** Add human judges alongside automated judging for style adherence
2. **More styles:** Expand beyond 13 styles to include more edge cases
3. **Cross-model comparison:** Test with additional model families (Mistral, Phi, etc.)

---

## 5. Conclusion

We successfully replicated Constitutional AI training starting from a base model, demonstrating that:

1. **CAI from base models works:** The approach achieves significant harm reduction (50.3% ASR decrease) while maintaining helpfulness.

2. **The methodology is reproducible:** Our implementation follows the original CAI paper's approach using few-shot prompting for pairwise comparisons without relying on instruction-tuned models.

3. **Style diversity hypothesis remains untested:** Due to methodological failures in our baseline comparison, we cannot evaluate whether base-model-derived CAI retains superior style flexibility.

The project specification's goal of creating a "de novo training procedure" that avoids implicit dependencies on existing assistant models has been achieved for the core CAI pipeline. The style diversity evaluation requires additional work to produce valid scientific conclusions.

---

## Appendix A: Detailed Per-Style Results

| Style | CAI Score | Instruct Score* | Notes |
|-------|-----------|-----------------|-------|
| technical_precise | 4.00 | 1.00* | Instruct: garbage output |
| noir_detective | 3.67 | 1.00* | Instruct: garbage output |
| formal_academic | 3.33 | 1.00* | Instruct: garbage output |
| poetic_literary | 3.33 | 1.00* | Instruct: garbage output |
| shakespeare_archaic | 3.00 | 1.00* | Instruct: garbage output |
| socratic_questioning | 2.00 | 1.00* | Instruct: garbage output |
| casual_conversational | 2.00 | 1.00* | Instruct: garbage output |
| grumpy_old_person | 1.67 | 1.00* | Instruct: garbage output |
| french_phrases | 1.67 | 1.00* | Instruct: garbage output |
| storytelling_narrative | 1.67 | 1.00* | Instruct: garbage output |
| enthusiastic_excited | 1.33 | 1.00* | Instruct: garbage output |
| minimalist_terse | 1.33 | 1.00* | Instruct: garbage output |
| overexcited_child | 1.00 | 1.00* | Tie (both failed) |

*\*Instruct scores are invalid due to model output failures*

---

## Appendix B: Code and Reproducibility

All code is available in the `cai-base-model/` directory:

- `config.py` - Experiment configuration and constitutional principles
- `cai_trainer.py` - Full training pipeline (SFT + DPO)
- `data_generation.py` - Constitutional pair generation
- `evaluation.py` - Harmlessness and helpfulness evaluation
- `style_diversity_eval.py` - Style evaluation (requires fixes for valid comparison)
- `run_experiment.py` - Main experiment runner

**To reproduce core CAI results:**
```bash
cd cai-base-model
python run_experiment.py --seeds 0 2 3 4 5 7 8 9
```

---

## References

1. Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. *arXiv:2212.08073*

2. Rafailov, R., et al. (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. *arXiv:2305.18290*

3. Thinking Machines Lab. (2025). Project Specification: Replicate Constitutional AI with Base Models. GitHub.

---

*Report generated: 2025-12-29*
*ü§ñ Generated with [Claude Code](https://claude.com/claude-code)*
